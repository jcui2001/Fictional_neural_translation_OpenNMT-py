{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/ymoslem/OpenNMT-Tutorial/blob/main/1-NMT-Data-Processing.ipynb","timestamp":1712884267331}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Gathering and Processing\n","\n","To build a Machine Translation system, you need bilingual data, i.e. source sentences and their translations. You can use public bilingual corpora/datasets or you can use your translation memories (TMs). However, NMT requires a lot of data to train a good model, that is why most companies start with training a strong baseline model using public bilingual datasets, and then fine-tune this baseline model on their TMs. Sometimes also you can use pre-trained models directly for fine-tuning.\n","\n","The majority of public bilingual datasets are collected on OPUS: https://opus.nlpl.eu/\n","\n","Most of the datasets can be used for both commercial and non-commercial uses; however, some of them have more restricted licences. So you have to double-check the licence of a dataset before using it.\n","\n","On OPUS, go to “Search & download resources” and choose two languages from the drop-down lists. You will see how it will list the available language datasets for this language pair. Try to use non-variant language codes like “en” for English and “fr” for French to get all the variants under this language. To know more details about a specific dataset, click its name.\n","\n","In Machine Translation, we use the “Moses” format. Go ahead and try to download the “tico-19 v2020-10-28” by clicking “moses”. This will download a *.zip file; when you extract it, the two files that you care about are those whose names ending by the language codes. For example, for English to French, you will have “tico-19.en-fr.en” and “tico-19.en-fr.fr“. You can open these files with any text editor. Each file has a sentence/segment per line, and it is matching translation in the same line in the other file. This is what the \"Moses\" file format means.\n","\n","Note that not all datasets are of the same quality. Some datasets have lower quality, especially big corpora crawled from the web. Check the provided “sample” before using the dataset. Nevertheless, even high-quality datasets, like those from the UN and EU, require filtering.\n","\n"],"metadata":{"id":"XMk-V8nT9IpU"}},{"cell_type":"code","source":["!cd ..\n","%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aXbbM-rg_OTo","executionInfo":{"status":"ok","timestamp":1714349699487,"user_tz":300,"elapsed":351,"user":{"displayName":"Jeremy Cuicui","userId":"11216180865255408651"}},"outputId":"3ef401f4-2cad-46b3-cd66-836850dd0a4f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34mnmt\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"]}]},{"cell_type":"code","metadata":{"id":"bLRpsedwqLg4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"58f1a82b-fb2d-4243-affd-04c1513b302f","executionInfo":{"status":"ok","timestamp":1714346192048,"user_tz":300,"elapsed":1447,"user":{"displayName":"Jeremy Cuicui","userId":"11216180865255408651"}}},"source":["# Create a directory and clone the Github MT-Preparation repository\n","'''!mkdir -p /content/drive/MyDrive/Colab\\ Notebook/Fictional\\ Neural\\ Translation/nmt\n","%cd /content/drive/MyDrive/Colab\\ Notebook/Fictional\\ Neural\\ Translation/nmt\n","!git clone https://github.com/ymoslem/MT-Preparation.git'''"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebook/Fictional Neural Translation/nmt\n","Cloning into 'MT-Preparation'...\n","remote: Enumerating objects: 268, done.\u001b[K\n","remote: Counting objects: 100% (268/268), done.\u001b[K\n","remote: Compressing objects: 100% (159/159), done.\u001b[K\n","remote: Total 268 (delta 133), reused 189 (delta 97), pack-reused 0\u001b[K\n","Receiving objects: 100% (268/268), 69.06 KiB | 1.11 MiB/s, done.\n","Resolving deltas: 100% (133/133), done.\n"]}]},{"cell_type":"code","metadata":{"id":"H8d13pqsp3Ii","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fe4ab5b2-2f97-4008-886b-2bec438868d9","executionInfo":{"status":"ok","timestamp":1714349736632,"user_tz":300,"elapsed":7755,"user":{"displayName":"Jeremy Cuicui","userId":"11216180865255408651"}}},"source":["# Install the requirements\n","!pip3 install -r /content/drive/MyDrive/Colab\\ Notebook/Fictional\\ Neural\\ Translation/nmt/MT-Preparation/requirements.txt"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebook/Fictional Neural Translation/nmt/MT-Preparation/requirements.txt (line 1)) (1.25.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebook/Fictional Neural Translation/nmt/MT-Preparation/requirements.txt (line 2)) (2.0.3)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r /content/drive/MyDrive/Colab Notebook/Fictional Neural Translation/nmt/MT-Preparation/requirements.txt (line 3)) (0.1.99)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /content/drive/MyDrive/Colab Notebook/Fictional Neural Translation/nmt/MT-Preparation/requirements.txt (line 2)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /content/drive/MyDrive/Colab Notebook/Fictional Neural Translation/nmt/MT-Preparation/requirements.txt (line 2)) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r /content/drive/MyDrive/Colab Notebook/Fictional Neural Translation/nmt/MT-Preparation/requirements.txt (line 2)) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r /content/drive/MyDrive/Colab Notebook/Fictional Neural Translation/nmt/MT-Preparation/requirements.txt (line 2)) (1.16.0)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PnIgddt43Ggd","executionInfo":{"status":"ok","timestamp":1714349759573,"user_tz":300,"elapsed":2346,"user":{"displayName":"Jeremy Cuicui","userId":"11216180865255408651"}},"outputId":"1882d8dd-b41f-4466-b532-4f4f82f56116"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"G903Vcm7u08U"},"source":["# Datasets\n","\n","Example datasets:\n","\n","* EN-AR: https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/ar-en.txt.zip\n","* EN-ES: https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-es.txt.zip\n","* EN-FR: https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-fr.txt.zip\n","* EN-RU: https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-ru.txt.zip\n","* EN-ZH: https://object.pouta.csc.fi/OPUS-UN/v20090831/moses/en-zh.txt.zip"]},{"cell_type":"markdown","source":["# Data Filtering\n","\n","Filtering out low-quality segments can help improve the translation quality of the output MT model. This might include misalignments, empty segments, duplicates, among other issues."],"metadata":{"id":"5G6GTlXa86Qb"}},{"cell_type":"code","metadata":{"id":"b-9jDIWarB-9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e0be3ca2-b9f8-4f61-9b96-1008f401ff3e","executionInfo":{"status":"ok","timestamp":1714351918910,"user_tz":300,"elapsed":1743,"user":{"displayName":"Jeremy Cuicui","userId":"11216180865255408651"}}},"source":["# Filter the dataset\n","# Arguments: source file, target file, source language, target language\n","!python3 /content/drive/MyDrive/Colab\\ Notebooks/Fictional\\ Neural\\ Translation/nmt/MT-Preparation/filtering/filter.py /content/drive/MyDrive/Colab\\ Notebooks/Fictional\\ Neural\\ Translation/nmt/en-tkn/UN.en-tkn.tkn /content/drive/MyDrive/Colab\\ Notebooks/Fictional\\ Neural\\ Translation/nmt/en-tkn/UN.en-tkn.en tkn en"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataframe shape (rows, columns): (1038, 2)\n","--- Rows with Empty Cells Deleted\t--> Rows: 1034\n","--- Duplicates Deleted\t\t\t--> Rows: 1025\n","--- Source-Copied Rows Deleted\t\t--> Rows: 1025\n","--- Too Long Source/Target Deleted\t--> Rows: 621\n","--- HTML Removed\t\t\t--> Rows: 621\n","--- Rows will remain in true-cased\t--> Rows: 621\n","--- Rows with Empty Cells Deleted\t--> Rows: 621\n","--- Rows Shuffled\t\t\t--> Rows: 621\n","--- Source Saved: /content/drive/MyDrive/Colab Notebooks/Fictional Neural Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn\n","--- Target Saved: /content/drive/MyDrive/Colab Notebooks/Fictional Neural Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en\n"]}]},{"cell_type":"markdown","source":["# Tokenization / Sub-wording\n","\n","To build a vocabulary for any NLP model, you have to tokenize (i.e. split) sentences into smaller units. Word-based tokenization used to be the way to go; in this case, each word would be a token. However, an MT model can only learn a specific number of vocabulary tokens due to limited hardware resources. To solve this issue, sub-words are used instead of whole words. At translation time, when the model sees a new word/token that looks like a word/token it has in the vocabulary, it still can try to continue the translation instead of marking this word as “unknown” or “unk”.\n","\n","There are a few approaches to sub-wording such as BPE and the unigram model. One of the famous toolkits that incorporates the most common approaches is [SentencePiece](https://github.com/google/sentencepiece). Note that you have to train a sub-wording model and then use it. After translation, you will have to “desubword” or “decode” your text back using the same SentencePiece model.\n","\n"],"metadata":{"id":"IbRpxXjC78c0"}},{"cell_type":"code","metadata":{"id":"n9c1pqhuru3j","colab":{"base_uri":"https://localhost:8080/"},"outputId":"89a1f3ab-2990-4bbc-f0fe-1852548c070a","executionInfo":{"status":"ok","timestamp":1714351957019,"user_tz":300,"elapsed":218,"user":{"displayName":"Jeremy Cuicui","userId":"11216180865255408651"}}},"source":["!ls /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/MT-Preparation/subwording/"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["1-train_bpe.py\t1-train_unigram.py  2-subword.py  3-desubword.py\n"]}]},{"cell_type":"code","metadata":{"id":"weSS6QDPsOUJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1a027d24-3520-4b2c-930b-3622697103ff","executionInfo":{"status":"ok","timestamp":1714352471734,"user_tz":300,"elapsed":925,"user":{"displayName":"Jeremy Cuicui","userId":"11216180865255408651"}}},"source":["# Train a SentencePiece model for subword tokenization\n","!python3 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/MT-Preparation/subwording/1-train_unigram.py /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn --model_prefix=source --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n","sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n","trainer_spec {\n","  input: /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn\n","  input_format: \n","  model_prefix: source\n","  model_type: UNIGRAM\n","  vocab_size: 50000\n","  self_test_sample_size: 0\n","  character_coverage: 0.9995\n","  input_sentence_size: 0\n","  shuffle_input_sentence: 1\n","  seed_sentencepiece_size: 1000000\n","  shrinking_factor: 0.75\n","  max_sentence_length: 4192\n","  num_threads: 16\n","  num_sub_iterations: 2\n","  max_sentencepiece_length: 16\n","  split_by_unicode_script: 1\n","  split_by_number: 1\n","  split_by_whitespace: 1\n","  split_digits: 1\n","  pretokenization_delimiter: \n","  treat_whitespace_as_suffix: 0\n","  allow_whitespace_only_pieces: 0\n","  required_chars: \n","  byte_fallback: 0\n","  vocabulary_output_piece_score: 1\n","  train_extremely_large_corpus: 0\n","  hard_vocab_limit: 0\n","  use_all_vocab: 0\n","  unk_id: 0\n","  bos_id: 1\n","  eos_id: 2\n","  pad_id: -1\n","  unk_piece: <unk>\n","  bos_piece: <s>\n","  eos_piece: </s>\n","  pad_piece: <pad>\n","  unk_surface:  ⁇ \n","  enable_differential_privacy: 0\n","  differential_privacy_noise_level: 0\n","  differential_privacy_clipping_threshold: 0\n","}\n","normalizer_spec {\n","  name: nmt_nfkc\n","  add_dummy_prefix: 1\n","  remove_extra_whitespaces: 1\n","  escape_whitespaces: 1\n","  normalization_rule_tsv: \n","}\n","denormalizer_spec {}\n","trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n","trainer_interface.cc(183) LOG(INFO) Loading corpus: /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn\n","trainer_interface.cc(407) LOG(INFO) Loaded all 621 sentences\n","trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n","trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n","trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n","trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n","trainer_interface.cc(537) LOG(INFO) all chars count=260202\n","trainer_interface.cc(548) LOG(INFO) Done: 99.9543% characters are covered.\n","trainer_interface.cc(558) LOG(INFO) Alphabet size=74\n","trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999543\n","trainer_interface.cc(591) LOG(INFO) Done! preprocessed 621 sentences.\n","unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n","unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=131795\n","unigram_model_trainer.cc(274) LOG(INFO) Initialized 17344 seed sentencepieces\n","trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 621\n","trainer_interface.cc(608) LOG(INFO) Done! 11116\n","unigram_model_trainer.cc(564) LOG(INFO) Using 11116 sentences for EM training\n","unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6555 obj=11.8929 num_tokens=22540 num_tokens/piece=3.4386\n","unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=5631 obj=9.80399 num_tokens=22610 num_tokens/piece=4.01527\n","trainer_interface.cc(686) LOG(INFO) Saving model: source.model\n","trainer_interface.cc(698) LOG(INFO) Saving vocabs: source.vocab\n","Done, training a SentencepPiece model for the Source finished successfully!\n","sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en --model_prefix=target --vocab_size=50000 --hard_vocab_limit=false --split_digits=true\n","sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n","trainer_spec {\n","  input: /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en\n","  input_format: \n","  model_prefix: target\n","  model_type: UNIGRAM\n","  vocab_size: 50000\n","  self_test_sample_size: 0\n","  character_coverage: 0.9995\n","  input_sentence_size: 0\n","  shuffle_input_sentence: 1\n","  seed_sentencepiece_size: 1000000\n","  shrinking_factor: 0.75\n","  max_sentence_length: 4192\n","  num_threads: 16\n","  num_sub_iterations: 2\n","  max_sentencepiece_length: 16\n","  split_by_unicode_script: 1\n","  split_by_number: 1\n","  split_by_whitespace: 1\n","  split_digits: 1\n","  pretokenization_delimiter: \n","  treat_whitespace_as_suffix: 0\n","  allow_whitespace_only_pieces: 0\n","  required_chars: \n","  byte_fallback: 0\n","  vocabulary_output_piece_score: 1\n","  train_extremely_large_corpus: 0\n","  hard_vocab_limit: 0\n","  use_all_vocab: 0\n","  unk_id: 0\n","  bos_id: 1\n","  eos_id: 2\n","  pad_id: -1\n","  unk_piece: <unk>\n","  bos_piece: <s>\n","  eos_piece: </s>\n","  pad_piece: <pad>\n","  unk_surface:  ⁇ \n","  enable_differential_privacy: 0\n","  differential_privacy_noise_level: 0\n","  differential_privacy_clipping_threshold: 0\n","}\n","normalizer_spec {\n","  name: nmt_nfkc\n","  add_dummy_prefix: 1\n","  remove_extra_whitespaces: 1\n","  escape_whitespaces: 1\n","  normalization_rule_tsv: \n","}\n","denormalizer_spec {}\n","trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n","trainer_interface.cc(183) LOG(INFO) Loading corpus: /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en\n","trainer_interface.cc(407) LOG(INFO) Loaded all 621 sentences\n","trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n","trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n","trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n","trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n","trainer_interface.cc(537) LOG(INFO) all chars count=286123\n","trainer_interface.cc(548) LOG(INFO) Done: 99.957% characters are covered.\n","trainer_interface.cc(558) LOG(INFO) Alphabet size=75\n","trainer_interface.cc(559) LOG(INFO) Final character coverage=0.99957\n","trainer_interface.cc(591) LOG(INFO) Done! preprocessed 621 sentences.\n","unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n","unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=149365\n","unigram_model_trainer.cc(274) LOG(INFO) Initialized 9327 seed sentencepieces\n","trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 621\n","trainer_interface.cc(608) LOG(INFO) Done! 6386\n","unigram_model_trainer.cc(564) LOG(INFO) Using 6386 sentences for EM training\n","unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3776 obj=10.3427 num_tokens=13369 num_tokens/piece=3.54052\n","unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3233 obj=8.40248 num_tokens=13448 num_tokens/piece=4.1596\n","trainer_interface.cc(686) LOG(INFO) Saving model: target.model\n","trainer_interface.cc(698) LOG(INFO) Saving vocabs: target.vocab\n","Done, training a SentencepPiece model for the Target finished successfully!\n"]}]},{"cell_type":"code","metadata":{"id":"T89THXeRslKu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c75a5287-d8b6-45ef-9eed-6831ec5ea7fc","executionInfo":{"status":"ok","timestamp":1714352511814,"user_tz":300,"elapsed":238,"user":{"displayName":"Jeremy Cuicui","userId":"11216180865255408651"}}},"source":["!ls /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["compute-bleu.py  en-fr\t models\t\t README  source.model  target.model  train.log\n","config.yaml\t en-tkn  MT-Preparation  run\t source.vocab  target.vocab\n"]}]},{"cell_type":"code","metadata":{"id":"hBWQoCfBsqlT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a3a53740-8591-46dd-bd9f-cb0621139d99","executionInfo":{"status":"ok","timestamp":1714352635726,"user_tz":300,"elapsed":1512,"user":{"displayName":"Jeremy Cuicui","userId":"11216180865255408651"}}},"source":["# Subword the dataset\n","!python3 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/MT-Preparation/subwording/2-subword.py /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/source.model /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/target.model /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en"],"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Source Model: /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/source.model\n","Target Model: /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/target.model\n","Source Dataset: /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn\n","Target Dataset: /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en\n","Done subwording the source file! Output: /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn.subword\n","Done subwording the target file! Output: /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en.subword\n"]}]},{"cell_type":"code","metadata":{"id":"CnfMRckbvNfZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1997ad66-3103-42ef-f9c2-0e111a758eaf","executionInfo":{"status":"ok","timestamp":1714352692094,"user_tz":300,"elapsed":244,"user":{"displayName":"Jeremy Cuicui","userId":"11216180865255408651"}}},"source":["# First 3 lines before subwording\n","!head -n 3 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn && echo \"-----\" && head -n 3 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["25 Ar yé! şanyengolmo oronte, tyastien se, ar eques: “*Peantar, mana caruvan náven aryon oira coiviéno?” 26 Quentes senna: “Mana técina i Şanyesse? Manen hental?” 27 Hanquentasse quentes: “Alye mele i Héru Ainolya quanda endalyanen ar quanda fealyanen ar quanda poldorelyanen ar quanda sámalyanen, ar armarolya ve imle.” 28 Yésus quente senna: “Hanquentes mai; cara sie ar samuval coivie.”\n","\t5 Ar íre ennoli caramper pa i corda, i samis írime ondor ar netyana annainen ná, quentes: 6 “Nati sine yar yétalde – aureli tuluvar yassen ondo ua lemyuva ondosse ya ua nauva hátina undu.”\n","Á vanta tienyasse, ve inye vanta i Hristo tiesse.\n","-----\n","21 At that time he was most happy by the Holy Spirit and said: “I praise you, Father, Lord over heaven and earth, for you have hidden these things from wise ones and from intelligent ones, and you have revealed them to babes. Yes, Father, for doing so was good in your eyes. 22 All things have been given over to me by my Father, and who the Son is nobody knows except the Father, and who the Father is, nobody knows except the Son, and anyone to whom the Son wishes to reveal him.”\n","Now when he looked he saw the rich putting their gifts in the treasuries. 2 Then he saw a poor widowed woman who threw in two small copper pieces, 3 and he said: “I tell you truly: This poor widowed woman gave more than them all. 4 For all these [people] gave gifts out of their abundance, but this woman gave the entire livelihood that she had.”\n","Walk my my path, as I walk in Christ's path.\n"]}]},{"cell_type":"code","metadata":{"id":"hs_xxKK_vf1Z","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0bda55d9-76d0-4a93-f036-4379aa2c18a8","executionInfo":{"status":"ok","timestamp":1714352730215,"user_tz":300,"elapsed":268,"user":{"displayName":"Jeremy Cuicui","userId":"11216180865255408651"}}},"source":["# First 3 lines after subwording\n","!head -n 3 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn.subword && echo \"---\" && head -n 3 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en.subword"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["▁ 2 5 ▁Ar ▁y é ! ▁ ş an y eng ol mo ▁o ront e , ▁t ya s tien ▁se , ▁a r ▁e ques : ▁ “* P e ant ar , ▁ man a ▁car u van ▁n á ve n ▁a r yon ▁ oir a ▁co iv i éno ?” ▁ 2 6 ▁ Q ue ntes ▁s en na : ▁ “ M ana ▁ té ci na ▁i ▁ Ş an y es s e ? ▁Man en ▁h ent al ?” ▁ 2 7 ▁H anque nt asse ▁que ntes : ▁ “ Al y e ▁me le ▁i ▁Hé r u ▁A in ol ya ▁qu anda ▁en d al yan en ▁a r ▁qu anda ▁f e al yan en ▁a r ▁qu anda ▁pol do re l yan en ▁a r ▁qu anda ▁s á ma l yan en , ▁a r ▁ arm a rol ya ▁ve ▁im le . ” ▁ 2 8 ▁ Y és us ▁que nt e ▁s en na : ▁ “ H anque ntes ▁mai ; ▁car a ▁si e ▁a r ▁sa m u val ▁co iv ie . ”\n","▁ 5 ▁Ar ▁ í re ▁en n oli ▁car am per ▁pa ▁i ▁ cord a , ▁i ▁sa mi s ▁ í rime ▁ ond or ▁a r ▁net yan a ▁an na in en ▁n á , ▁que ntes : ▁ 6 ▁ “ N ati ▁si ne ▁y ar ▁y étal de ▁ – ▁au re li ▁t ulu va r ▁y asse n ▁on do ▁ u a ▁le m y u va ▁on do sse ▁y a ▁ u a ▁n au va ▁h á tin a ▁un du . ”\n","▁ Á ▁ vant a ▁ tien y asse , ▁ve ▁in y e ▁ vant a ▁i ▁H rist o ▁ ties s e .\n","---\n","▁ 2 1 ▁At ▁that ▁time ▁he ▁was ▁most ▁h app y ▁by ▁the ▁Ho ly ▁S pirit ▁and ▁ s aid : ▁ “ I ▁p raise ▁ y ou , ▁Fa ther , ▁Lo rd ▁over ▁heav en ▁and ▁earth , ▁for ▁ y ou ▁have ▁hi dden ▁the se ▁things ▁from ▁wi se ▁on es ▁and ▁from ▁in te lli gen t ▁on es , ▁and ▁ y ou ▁have ▁reveal ed ▁them ▁to ▁ba b es . ▁Y es , ▁Fa ther , ▁for ▁doing ▁so ▁was ▁good ▁in ▁ y our ▁ ey es . ▁ 2 2 ▁All ▁things ▁have ▁been ▁given ▁over ▁to ▁me ▁by ▁m y ▁Fa ther , ▁and ▁who ▁the ▁S on ▁is ▁no bod y ▁know s ▁except ▁the ▁Fa ther , ▁and ▁who ▁the ▁Fa ther ▁is , ▁no bod y ▁know s ▁except ▁the ▁S on , ▁and ▁any one ▁to ▁whom ▁the ▁S on ▁wishes ▁to ▁reveal ▁him . ”\n","▁No w ▁when ▁he ▁look ed ▁he ▁ s aw ▁the ▁rich ▁put ting ▁their ▁gift s ▁in ▁the ▁trea s ur ies . ▁ 2 ▁The n ▁he ▁ s aw ▁a ▁poor ▁wid owed ▁woman ▁who ▁thre w ▁in ▁two ▁small ▁cop per ▁ pie ces , ▁ 3 ▁and ▁he ▁ s aid : ▁ “ I ▁t ell ▁ y ou ▁tru ly : ▁Thi s ▁poor ▁wid owed ▁woman ▁ga ve ▁more ▁than ▁them ▁all . ▁ 4 ▁For ▁all ▁the se ▁ [ people ] ▁ga ve ▁gift s ▁out ▁of ▁their ▁ abunda nce , ▁ but ▁this ▁woman ▁ga ve ▁the ▁entire ▁livelihood ▁that ▁she ▁had . ”\n","▁W al k ▁m y ▁m y ▁path , ▁as ▁I ▁ wal k ▁in ▁Christ ' s ▁path .\n"]}]},{"cell_type":"markdown","source":["# Data Splitting\n","\n","We usually split our dataset into 3 portions:\n","\n","1. training dataset - used for training the model;\n","2. development dataset - used to run regular validations during the training to help improve the model parameters; and\n","3. testing dataset - a holdout dataset used after the model finishes training to finally evaluate the model on unseen data."],"metadata":{"id":"YgTZ-m718neI"}},{"cell_type":"code","metadata":{"id":"KfQRMGRixBAL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"85875f43-5190-40cc-d90c-7b778a52221c","executionInfo":{"status":"ok","timestamp":1714353283522,"user_tz":300,"elapsed":926,"user":{"displayName":"Jeremy Cuicui","userId":"11216180865255408651"}}},"source":["# Split the dataset into training set, development set, and test set\n","# Development and test sets should be between 1000 and 5000 segments (here we chose 2000)\n","!python3 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/MT-Preparation/train_dev_split/train_dev_test_split.py 600 600 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn.subword /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en.subword"],"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataframe shape: (621, 2)\n","--- Empty Cells Deleted --> Rows: 621\n","--- Wrote Files\n","Done!\n","Output files\n","/content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn.subword.train\n","/content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en.subword.train\n","/content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn.subword.dev\n","/content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en.subword.dev\n","/content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn.subword.test\n","/content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en.subword.test\n"]}]},{"cell_type":"code","metadata":{"id":"6y3HQr4nxYib","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fcbc99b9-2322-4073-ff68-ebd9c2465765","executionInfo":{"status":"ok","timestamp":1714353339010,"user_tz":300,"elapsed":259,"user":{"displayName":"Jeremy Cuicui","userId":"11216180865255408651"}}},"source":["# Line count for the subworded train, dev, test datatest\n","!wc -l /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/*.subword.*"],"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["    600 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en.subword.dev\n","    600 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en.subword.test\n","     17 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en.subword.train\n","    600 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn.subword.dev\n","    600 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn.subword.test\n","     17 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn.subword.train\n","   2434 total\n"]}]},{"cell_type":"code","metadata":{"id":"0duUCLP93GKE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7069f6e6-c633-443d-9d5b-974cbf2c9d1a","executionInfo":{"status":"ok","timestamp":1714353431638,"user_tz":300,"elapsed":635,"user":{"displayName":"Jeremy Cuicui","userId":"11216180865255408651"}}},"source":["# Check the first and last line from each dataset\n","\n","# -------------------------------------------\n","# Change this cell to print your name\n","!echo -e \"My name is: FirstName SecondName \\n\"\n","# -------------------------------------------\n","\n","!echo \"---First line---\"\n","!head -n 1 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/*.{train,dev,test}\n","\n","!echo -e \"\\n---Last line---\"\n","!tail -n 1 /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/*.{train,dev,test}"],"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["My name is: FirstName SecondName \n","\n","---First line---\n","==> /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en.subword.train <==\n","▁ 2 5 ▁Wo e ▁to ▁ y ou ▁that ▁are ▁full ▁now , ▁for ▁ y ou ▁will ▁be ▁hung ry !\n","\n","==> /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn.subword.train <==\n","▁H or ro ▁l en ▁i ▁la la r ▁s í , ▁an ▁sa m u val de ▁n y é re ▁a r ▁n í ri !\n","\n","==> /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en.subword.dev <==\n","▁ 3 2 ▁The ▁P har ise es ▁hear d ▁the ▁ c row d ▁mu rm uring ▁the se ▁things ▁ about ▁him , ▁and ▁the ▁chief ▁pr ies ts ▁and ▁the ▁P har ise es ▁sent ▁some ▁officers ▁to ▁seize ▁him . ▁ 3 3 ▁T herefore ▁Je sus ▁ s aid : ▁ “ I ▁will ▁ still ▁be ▁with ▁ y ou ▁a ▁short ▁time , ▁before ▁I ▁shall ▁go ▁a way ▁to ▁ [ the ▁one ] ▁who ▁sent ▁me . ▁ 3 4 ▁You ▁will ▁seek ▁me , ▁ but ▁ y ou ▁will ▁not ▁find ▁me , ▁for ▁where ▁I ▁a m ▁ y ou ▁can not ▁come . ” ▁ 3 5 ▁T herefore ▁the ▁Jews ▁ s aid ▁among ▁themselves : ▁ “ Where ▁will ▁this ▁man ▁go ▁a way ▁to , ▁that ▁we ▁will ▁not ▁find ▁him ? ▁Su rely ▁he ▁will ▁not ▁go ▁a way ▁to ▁ [ th ose ] ▁who ▁are ▁sc atter ed ▁among ▁the ▁Gree k , ▁in ▁order ▁to ▁teach ▁the ▁Gree k ? ▁ 3 6 ▁W hat ▁do es ▁this ▁ s aying ▁mean ▁that ▁he ▁ s aid : ▁You ▁will ▁seek ▁me , ▁ but ▁ y ou ▁will ▁not ▁find ▁me , ▁and ▁where ▁I ▁a m ▁ y ou ▁can not ▁come ▁ – ▁ ?”\n","\n","==> /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn.subword.dev <==\n","▁ 2 8 ▁E tt a ▁ Y és us , ▁ í re ▁pe ant an es ▁i ▁ cord asse , ▁et y á me ▁a r ▁que nt e : ▁ “ N í ▁i sta ld e , ▁y ú ▁i sta ld e ▁y all o ▁n an y e . ▁L á ▁ ut ú l ien ▁im n in en , ▁mal ▁y e ▁ni - ment ane ▁ ş anda ▁n á , ▁sé ▁y e ▁el de ▁ u ar ▁i sta . ▁ 2 9 ▁In y e ▁i sta ▁se , ▁an ▁se l lo ▁t ul in , ▁a r ▁sé ▁ ment ane ▁ni . ” ▁ 3 0 ▁T á ▁né ve l te ▁m apa ▁se , ▁mal ▁ *ú qu en ▁p an yan e ▁m á r ya ▁ses s e , ▁an ▁l ú mer ya ▁en ▁ ú me ▁t ú l ien wa . ▁ 3 1 ▁Mal ▁ ri mba li ▁i ▁ ş ang o ▁s á ver ▁ses s e ▁a r ▁que nt er : ▁ “Í re ▁i ▁H rist o ▁t ulu va , ▁l au ▁car u va s ▁c ard ar ▁ r im be ▁l á ▁y ar ▁ac á rie ▁né r ▁si na ?”\n","\n","==> /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en.subword.test <==\n","▁At ▁the ▁same ▁moment ▁his ▁m out h ▁was ▁open ed ▁and ▁his ▁to ng ue ▁loo s ened , ▁and ▁he ▁spoke , ▁b less ing ▁G od . ▁ 6 5 ▁And ▁fear ▁fell ▁upon ▁all ▁his ▁ nei gh b ors , ▁and ▁in ▁the ▁whole ▁mountain - region ▁in ▁Ju de a ▁one ▁t al k ed ▁ about ▁all ▁the se ▁things , ▁ 6 5 ▁and ▁all ▁that ▁hear d ▁per ceived ▁it ▁in ▁their ▁heart , ▁ s aying : ▁ “ W hat ▁in de ed ▁will ▁this ▁child ▁be ?” ▁For ▁also ▁the ▁Lo rd ' s ▁hand ▁was ▁with ▁him .\n","\n","==> /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn.subword.test <==\n","▁ 6 4 ▁M í ▁im ya ▁l ú ▁ ant or ya ▁n á ne ▁p ant ana ▁a r ▁l amb er ya ▁le ht ana , ▁a r ▁que ntes , ▁a ista la ▁E r u . ▁ 6 5 ▁Ar ▁ru ci e ▁l ant ane ▁il y e ▁ arm a ror yan na r , ▁a r ▁m í ▁qu anda ▁ ort om éna ▁ Yú re asse ▁mo ▁car am per ▁pa ▁il y e ▁n ati ▁si ne , ▁ 6 5 ▁a r ▁ illi ▁i ▁h l asser ▁t ú ner ▁sa ▁en d alt asse , ▁que t il a : ▁ “ É ▁ man a ▁n au va ▁h í na ▁si na ?” ▁A n ▁y ú ▁i ▁Hé r u o ▁m á ▁en ge ▁ ó s e .\n","\n","---Last line---\n","==> /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en.subword.train <==\n","▁day ▁after ▁day ▁the y ▁were ▁in ▁the ▁ te mple , ▁being ▁of ▁one ▁mind , ▁and ▁the y ▁broke ▁brea d ▁in ▁the ▁houses , ▁ eating ▁their ▁food ▁in ▁ j o y ▁and ▁having ▁a ▁pur e ▁heart , ▁ 4 7 ▁p raising ▁G od ▁and ▁having ▁the ▁gra ce ▁ [ or , ▁good ▁will ] ▁of ▁the ▁entire ▁people , ▁while ▁every ▁day ▁the ▁Lo rd ▁was ▁add ing ▁to ▁them ▁ [ th ose ] ▁that ▁were ▁sav ed .\n","\n","==> /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn.subword.train <==\n","▁ 4 3 ▁R u ci e ▁l ant ane ▁il qu en na , ▁a r ▁ r im be ▁el men d ali ▁a r ▁t an w ali ▁mar tan er ▁ ter ▁i ▁a po ste li . ▁ 4 4 ▁I lli ▁i ▁s á ver ▁n á ner ▁ u o ▁sa mi es s e ▁il qu a ▁ * ala s at ya , ▁ 4 5 ▁a r ▁v ance l te ▁ arm alt ar ▁a r ▁i ▁rest ar ▁y ar ▁ har yan el te ▁a r ▁et s ant er ▁i ▁tel pe ▁i llin , ▁a ique no ▁ma ur en en . ▁ 4 6 ▁Ar ▁au re ▁ apa ▁au re ▁a nel te ▁i ▁ cord asse , ▁n á la ▁ er ▁s á mo , ▁a r ▁ r ance l te ▁ma s s a ▁i ▁co asse n , ▁mati la ▁mat tal ta ▁mi ▁al asse ▁a r ▁a rw e ▁p oi ca ▁endo , ▁ 4 7 ▁l ait ala ▁E r u ▁a r ▁a rw e ▁i ▁ liss e o ▁i ▁qu anda ▁l ie o , ▁la n ▁il ya ▁au resse ▁i ▁Her u ▁n ap á na ne ▁ tien ▁i ▁n á ner ▁re ht ane .\n","\n","==> /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en.subword.dev <==\n","▁ 4 ▁ [ The re ] ▁exist ▁numer ous ▁kind s ▁of ▁gift s , ▁ but ▁the ▁spirit ▁is ▁the ▁same . ▁ 5 ▁ [ The re ] ▁exist ▁numer ous ▁kind s ▁of ▁servant - work ▁ [ ministr y ] , ▁ but ▁the ▁Lo rd ▁is ▁the ▁same . ▁ 6 ▁ [ The re ] ▁exist ▁numer ous ▁kind s ▁of ▁work , ▁ but ▁the ▁same ▁G od ▁work s ▁every thing ▁in ▁everyone . ▁ 7 ▁Bu t ▁the ▁show ing ▁of ▁the ▁spirit ▁is ▁given ▁to ▁everyone ▁for ▁a ▁beneficial ▁purpose . ▁ 8 ▁To ▁one ▁person ▁is ▁given ▁speech ▁of ▁wis dom , ▁to ▁another ▁speech ▁of ▁knowledge , ▁as ▁the ▁same ▁spirit ▁lead s ▁him ; ▁ 9 ▁to ▁another , ▁faith ▁from ▁the ▁same ▁spirit , ▁to ▁another , ▁gift s ▁of ▁heal ing ▁by ▁the ▁sole ▁spirit , ▁ 1 0 ▁to ▁another , ▁working ▁of ▁w on d ers , ▁to ▁another , ▁ speaking ▁as ▁a ▁Pro ph et , ▁to ▁another ▁in s ight ▁on ▁the ▁spirit s , ▁to ▁another , ▁ [ v arious ] ▁kind s ▁of ▁to n gues , ▁to ▁another , ▁t elling ▁what ▁to n gues ▁mean . ▁ 1 1 ▁Bu t ▁all ▁the se ▁things ▁are ▁being ▁work ed ▁by ▁the ▁one ▁and ▁the ▁same ▁spirit , ▁distribut ing ▁them ▁as ▁it ▁wishes ▁to ▁every ▁person .\n","\n","==> /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn.subword.dev <==\n","▁ 4 ▁E ar ▁ r im be ▁no sta l éli on ▁an na r , ▁mal ▁i ▁faire ▁i ▁im ya ▁n á . ▁ 5 ▁E ar ▁ r im be ▁no sta l éli on ▁n ú rom oli e , ▁mal ▁i ▁Her u ▁i ▁im ya ▁n á . ▁ 6 ▁E ar ▁ r im be ▁no sta l éli on ▁mo lie , ▁mal ▁i ▁im ya ▁A in o ▁mo le ▁il qu a ▁mi ▁il qu en . ▁ 7 ▁Mal ▁i ▁t an á ve ▁i ▁faire va ▁n á ▁ ant ana ▁il qu en en ▁a ş e a ▁en n en . ▁ 8 ▁E r ▁qu en en ▁n á ▁ ant ana ▁que sta ▁sa il i éva , ▁ex en ▁que sta ▁i s ty ava , ▁ve ▁i ▁im ya ▁faire ▁t ul ya ▁se ; ▁ 9 ▁ex en , ▁sa vie ▁i ▁im ya ▁faire l lo ; ▁ex en , ▁an na r ▁n esti éva ▁i ▁ er ya ▁faire n en ; ▁ 1 0 ▁ex en , ▁mo lie ▁el m enda iva , ▁ex en , ▁que t ie ▁ve ▁E r uter c á no , ▁ex en , ▁t erc en ▁i ▁fai ris s en , ▁ex en , ▁no sta ler ▁la mbi on , ▁ex en , ▁n y arie ▁ man a ▁la mbi ▁ te ar . ▁ 1 1 ▁Mal ▁il y e ▁n ati ▁si ne ▁n ar ▁m ó l aine ▁l o ▁i ▁ er ▁a r ▁i ▁im ya ▁faire , ▁et s á t ala ▁ta i ▁ve ▁mer is ▁il ya ▁qu en en .\n","\n","==> /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.en-filtered.en.subword.test <==\n","▁ CH AP T ER ▁ 4\n","\n","==> /content/drive/MyDrive/Colab_Notebooks/Fictional_Neural_Translation/nmt/en-tkn/UN.en-tkn.tkn-filtered.tkn.subword.test <==\n","▁R A N T A ▁ 4\n"]}]}]}